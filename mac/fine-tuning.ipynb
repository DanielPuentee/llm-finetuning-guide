{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea4a084",
   "metadata": {},
   "source": [
    "*Author: [Daniel Puente Viejo](https://www.linkedin.com/in/danielpuenteviejo/)*\n",
    "\n",
    "## **Mac - Fine-Tuning LLMs: A Practical Guide**\n",
    "\n",
    "A practical guide to fine-tuning LLMs (TinyLlama-1.1B) using a local adaptation on Mac (MPS) using PEFT.\n",
    "\n",
    "‚ö†Ô∏è **Disclaimer**: As Mac devices are not designed for cuda-based training, the performance may slower than expected. Nonetheless, we will be using the MPS (Metal Performance Shaders) backend to leverage the GPU capabilities of Mac devices. In this guide, we will be using the TinyLlama-1.1B model, and PEFT for local fine-tuning. Libraries like Unsloth are faster, but they are not compatible with MPS.\n",
    "\n",
    "üìä **Data:** The data used in this example is a synthetic data file generated about the history of basketball.\n",
    "\n",
    "### **Index:**\n",
    "\n",
    "- <a href='#1'><ins>1. SetUp</ins></a>\n",
    "    - <a href='#1.1'><ins>1.1 Libraries</ins></a>\n",
    "    - <a href='#1.2'><ins>1.2 Environment Variables</ins></a>\n",
    "- <a href='#2'><ins>2. Testing TinyLlama</ins></a>\n",
    "    - <a href='#2.1'><ins>2.1 Configuration</ins></a>\n",
    "    - <a href='#2.2'><ins>2.2 Ways of using the model</ins></a>\n",
    "        - <a href='#2.2.1'><ins>2.2.1 Using the pipeline</ins></a>\n",
    "        - <a href='#2.2.2'><ins>2.2.2 Using the model and tokenizer directly</ins></a>\n",
    "- <a href='#3'><ins>3. Fine-Tuning</ins></a>\n",
    "    - <a href='#3.1'><ins>3.1 Configuration</ins></a>\n",
    "    - <a href='#3.2'><ins>3.2 Load Dataset</ins></a>\n",
    "        - <a href='#3.2.1'><ins>3.2.1 Load the whole text at once</ins></a>\n",
    "        - <a href='#3.2.2'><ins>3.2.2 Load the text in chunks</ins></a>\n",
    "        - <a href='#3.2.3'><ins>3.2.3 Load dataset from JSON</ins></a>\n",
    "    - <a href='#3.3'><ins>3.3 Load the tokenizer and model</ins></a>\n",
    "    - <a href='#3.4'><ins>3.4 LoRA Configuration</ins></a>\n",
    "    - <a href='#3.5'><ins>3.5 SFT Configuration</ins></a>\n",
    "    - <a href='#3.6'><ins>3.6 Training the model</ins></a>\n",
    "- <a href='#4'><ins>4. Try the fine-tuned model</ins></a>\n",
    "    - <a href='#4.1'><ins>4.1 Load the fine-tuned model</ins></a>\n",
    "    - <a href='#4.2'><ins>4.2 Test the model</ins></a>\n",
    "        - <a href='#4.2.1'><ins>4.2.1 Fine-tuned model using the pipeline</ins></a>\n",
    "        - <a href='#4.2.2'><ins>4.2.2 Fine-tuned model using the model and tokenizer directly</ins></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a54f70",
   "metadata": {},
   "source": [
    "## <a id='1' style=\"color: skyblue;\">**1. Setup**</a>\n",
    "\n",
    "###  <a id='1.1'>**1.1 Libraries**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670429f8",
   "metadata": {},
   "source": [
    "Install the requirements\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9a9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from loguru import logger\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from peft import LoraConfig\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cdca6",
   "metadata": {},
   "source": [
    "###  <a id='1.2'>**1.2 Environment Variables**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01dc6914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daef82a",
   "metadata": {},
   "source": [
    "## <a id='2' style=\"color: skyblue;\">**2. Testing TinyLlama**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40e355",
   "metadata": {},
   "source": [
    "###  <a id='2.1'>**2.1 Configuration**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d31dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "device = \"mps\" # Use \"cuda\" if you have an NVIDIA GPU, \"mps\" for Apple Silicon, or \"cpu\" as a fallback.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Suppress tokenizer parallelism warnings. This is optional but can help reduce noise in the output.\n",
    "\n",
    "query = \"\"\"How many players were on each team in the very first basketball game?\"\"\" \n",
    "expected_answer = \"9 players per team + coach\"\n",
    "# Response: 9 players per team + coach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4595246",
   "metadata": {},
   "source": [
    "###  <a id='2.2'>**2.2 Ways of using the model**</a>\n",
    "\n",
    "There are two main ways to use the model:\n",
    "1. Through the Hugging Face pipeline\n",
    "2. By using the model and tokenizer directly. \n",
    "\n",
    "The pipeline is simpler and abstracts away many details, while using the model and tokenizer directly gives you more control.\n",
    "\n",
    "####  <a id='2.2.1'>**2.2.1 Using the pipeline**</a>\n",
    "\n",
    "‚ùå As you can see, the model generates an answer but it is not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee089692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\t There were two teams in the very first basketball game: the Boston Celtics and the Philadelphia Warriors. Both teams had five players on their roster.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Expected: 9 players per team + coach\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_name, \n",
    "    dtype=torch.bfloat16, \n",
    "    device_map=device\n",
    "\n",
    "    # Speed optimizations:\n",
    "    # batch_size=1,           # Adjust if processing multiple prompts\n",
    "    # use_cache=True,         # Enable KV-cache (usually default)\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot that answer basketball questions.\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "# Pass messages directly - cleaner!\n",
    "outputs = pipe(\n",
    "    messages, \n",
    "    max_new_tokens=256, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "\n",
    "    ### Speed tricks:\n",
    "    # pad_token_id=pipe.tokenizer.eos_token_id,  # Avoid padding warnings\n",
    "    return_full_text=False, \n",
    ")\n",
    "answer = outputs[0][\"generated_text\"]\n",
    "print(\"Answer:\\t\", answer)\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(\"Expected:\", expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e977b0",
   "metadata": {},
   "source": [
    "#### <a id='2.2.2'>**2.2.2 Using the model and tokenizer directly**</a>\n",
    "\n",
    "‚ùå As you can see, the model generates an answer but it is not good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ae5b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\t The very first basketball game had only two players on each team.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Expected: 9 players per team + coach\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16, \n",
    "    device_map=device # Mac GPU\n",
    ")\n",
    "\n",
    "prompt = f\"<|user|>\\nRETRIEVE: {query}</s>\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "raw_data = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "retrieved_fact = raw_data.split(\"<|assistant|>\")[-1].strip()\n",
    "print(\"Answer:\\t\", retrieved_fact)\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(\"Expected:\", expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53cb20",
   "metadata": {},
   "source": [
    "## <a id='3' style=\"color: skyblue;\">**3. Fine-Tuning**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2ea8fa",
   "metadata": {},
   "source": [
    "###  <a id='3.1'>**3.1 Configuration**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e69dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/data.txt\"\n",
    "atomic_data_file = \"../data/atomic_train.json\"\n",
    "\n",
    "new_model_name = \"tiny-llama-finetuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ab9e8",
   "metadata": {},
   "source": [
    "###  <a id='3.2'>**3.2 Load Dataset**</a>\n",
    "\n",
    "There are 3 ways to load the dataset. \n",
    "1. One is to load the **whole dataset at once**.\n",
    "2. The second is to load the **dataset in chunks**.\n",
    "3. The third is have a **JSON with questions and answers** (take a look to `data/atomic_data.txt`). This can be done passing to a LLM all the text and construction this JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee99d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pirate_format(example):\n",
    "    text = example['text']\n",
    "    # We repeat the pirate persona instructions so it associates this style with the facts\n",
    "    formatted = (\n",
    "        \"<|system|>\\n\"\n",
    "        \"You are a friendly chatbot who always responds in the style of a pirate.</s>\\n\"\n",
    "        \"<|user|>\\n\"\n",
    "        \"Tell me a fact about basketball.</s>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        f\"{text}</s>\"\n",
    "    )\n",
    "    return {\"text\": formatted}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355cbd9",
   "metadata": {},
   "source": [
    "#### <a id='3.2.1'>**3.2.1 Load whole text at once**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab10645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-08 18:58:39.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mFormatting dataset...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": data_file})\n",
    "\n",
    "# Apply the formatting immediately\n",
    "logger.info(\"Formatting dataset...\")\n",
    "dataset[\"train\"] = dataset[\"train\"].map(apply_pirate_format)\n",
    "\n",
    "# Filter out empty lines just in case\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x[\"text\"] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd70f1d",
   "metadata": {},
   "source": [
    "#### <a id='3.2.2'>**3.2.2 Load text in chunks**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4134739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e27d4104ca9432baf4271d23a592e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(data_file, \"r\") as f:\n",
    "    raw_text_chunks = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Create dataset from chunks\n",
    "dataset = Dataset.from_dict({\"text\": raw_text_chunks})\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset\n",
    "})\n",
    "\n",
    "# Apply formatting immediately\n",
    "print(\"Formatting dataset...\")\n",
    "dataset = dataset.map(apply_pirate_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffcccc",
   "metadata": {},
   "source": [
    "#### <a id='3.2.3'>**3.2.3 Load dataset from JSON**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662a5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=atomic_data_file)\n",
    "\n",
    "def format_for_retrieval(example):\n",
    "    # We use special tokens to mark the query and data clearly\n",
    "    formatted = (\n",
    "        \"<|user|>\\n\"\n",
    "        f\"RETRIEVE: {example['question']}</s>\\n\"\n",
    "        \"<|assistant|>\\n\"\n",
    "        f\"{example['answer']}</s>\"\n",
    "    )\n",
    "    return {\"text\": formatted}\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(format_for_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79bf583",
   "metadata": {},
   "source": [
    "### <a id='3.3'>**3.3 Load the tokenizer and model**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac045a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16, \n",
    "    device_map=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd01b0",
   "metadata": {},
   "source": [
    "### <a id='3.4'>**3.4 LoRA Configuration**</a>\n",
    "\n",
    "What is LoRA? LoRA (Low-Rank Adaptation) is a technique for fine-tuning large language models that reduces the number of trainable parameters by decomposing the weight updates into low-rank matrices. This allows for efficient fine-tuning on smaller datasets and with limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041e2471",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 16\n",
    "peft_config = LoraConfig(\n",
    "    # Optimal: Start with 8 or 16. Use 32 or 64 for complex tasks. \n",
    "    # Warning: Higher 'r' increases VRAM usage slightly and training file size.\n",
    "    r=r, # The size of the adapter matrices. Controls how \"smart\" the fine-tuning is vs memory usage.\n",
    "\n",
    "    # Optimal: Standard rule of thumb is alpha = 2 * r (so here, 32).\n",
    "    # If you find the model \"forgets\" how to speak English, lower this.\n",
    "    lora_alpha=r*2, # Scaling factor for the weights. Determines how much influence the new LoRA weights have over the old model weights.\n",
    "\n",
    "    # Optimal: 0.05 (5%) or 0.1 (10%) are standard.\n",
    "    lora_dropout=0.1, # Randomly disables neurons during training to prevent overfitting.\n",
    "\n",
    "    # Impact: \"none\" saves the most memory and is standard for LoRA.\n",
    "    # Optimal: \"none\" (unless you have a very specific reason to train biases).\n",
    "    bias=\"none\", # Whether to train bias parameters.\n",
    "\n",
    "    # Optimal: Always \"CAUSAL_LM\" for text generation models (Llama, Mistral, GPT).\n",
    "    task_type=\"CAUSAL_LM\", # Tells LoRA what kind of model this is.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac0e07",
   "metadata": {},
   "source": [
    "### <a id='3.5'>**3.5 SFT Configuration**</a>\n",
    "\n",
    "SFT (Supervised Fine-Tuning) is a method of fine-tuning large language models using supervised learning. It involves training the model on a labeled dataset, where the input data is paired with the corresponding output labels. This allows the model to learn from the specific examples in the dataset and improve its performance on similar tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4ce3162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This controls the training loop (speed, memory, duration).\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "\n",
    "    # - 1 to 3: For large datasets (thousands of examples).\n",
    "    # - 5 to 10: For very small datasets to ensure it learns.\n",
    "    num_train_epochs=15, # How many times the model sees your entire dataset.\n",
    "\n",
    "    # - 1: For Mac/MPS (crucial to avoid Out of Memory crashes).\n",
    "    # - 2 or 4: Only if you have a massive GPU (A100/H100).\n",
    "    per_device_train_batch_size=1, # How many examples to process at once per GPU.\n",
    "\n",
    "    # Effective Batch Size = batch_size * grad_acc_steps (1 * 4 = 4).\n",
    "    # Optimal: Aim for an effective batch size of 16 or 32. \n",
    "    # If batch_size is 1, set this to 4, 8, or 16 depending on patience/memory.\n",
    "    gradient_accumulation_steps=4, # \"Fake\" batch size.  It waits this many steps before updating weights. \n",
    "\n",
    "    # - 2e-4: Standard \"Sweet Spot\" for LoRA.\n",
    "    learning_rate=2e-4, # How fast the model updates its brain.\n",
    "\n",
    "    # Optimal: 10 is fine. For tiny datasets, maybe 1 or 5 to see progress fast.\n",
    "    logging_steps=10, # How often to print stats (loss) to the console.\n",
    "\n",
    "    save_strategy=\"epoch\", # When to save a checkpoint.\n",
    "\n",
    "    # Optimal: True (CRITICAL for Mac M1/M2/M3 chips for speed and stability).\n",
    "    # If on an old Intel Mac or old NVIDIA GPU, use fp16=True instead.\n",
    "    bf16=True, # Use Brain Floating Point 16.\n",
    "\n",
    "    # Optimal: False for beginners/small data. True for massive training runs to save time.\n",
    "    packing=False, # crams multiple short examples into one long sequence.\n",
    "\n",
    "    # - 512: Good for short Q&A. Saves massive amounts of memory.\n",
    "    # - 1024 or 2048: Use only if your text examples are long essays.\n",
    "    max_length=512, # The maximum tokens the model can read/write in one go during training.\n",
    "\n",
    "    # Note: Only used if we formatted the data *before* the trainer (which we did manually).\n",
    "    dataset_text_field=\"text\" # Column name in your data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4309b01",
   "metadata": {},
   "source": [
    "### <a id='3.6'>**3.6 Train the model**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23703399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "\u001b[32m2026-02-08 18:59:00.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mStarting training on Mac (MPS)...\u001b[0m\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 15:45, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.980600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.993800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.898100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.867400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.854900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.783500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.752900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.564600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.611300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.481100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.391400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-08 19:14:47.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mSaving to tiny-llama-finetuned...\u001b[0m\n",
      "\u001b[32m2026-02-08 19:14:47.495\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[32m\u001b[1mDone!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "\n",
    "# Train and Save\n",
    "logger.info(\"Starting training on Mac (MPS)...\")\n",
    "trainer.train()\n",
    "\n",
    "logger.info(f\"Saving to {new_model_name}...\")\n",
    "trainer.model.save_pretrained(new_model_name)\n",
    "tokenizer.save_pretrained(new_model_name)\n",
    "logger.success(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06060dd8",
   "metadata": {},
   "source": [
    "## <a id='4' style=\"color: skyblue;\">**4. Try the fine-tuned model**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206895f",
   "metadata": {},
   "source": [
    "### <a id='4.1'>**4.1 Load the fine-tuned model**</a>\n",
    "We load the base model and attach the fine-tuned adapter. Then we create a text generation pipeline to test the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68d1557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading pirate adapter...\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the Base Model (MPS Optimized)\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "# 3. Load and Attach the Pirate Adapter\n",
    "print(\"Loading pirate adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)\n",
    "\n",
    "# 4. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210871ba",
   "metadata": {},
   "source": [
    "### <a id='4.2'>**4.2 Test the model**</a>\n",
    "\n",
    "Here again there are 2 ways to test the model:\n",
    "1. Through the Hugging Face pipeline\n",
    "2. By using the model and tokenizer directly.\n",
    "\n",
    "We will use the second option, but you can test it with the pipeline if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9859699",
   "metadata": {},
   "source": [
    "#### <a id='4.2.1'>**4.2.2 Fine-tuned model using the model and tokenizer directly**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee894107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\t The first basketball game had 9 players per team plus a coach.\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "Expected: 9 players per team + coach\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"<|user|>\\nRETRIEVE: {query}</s>\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.0,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "raw_data = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "retrieved_fact = raw_data.split(\"<|assistant|>\")[-1].strip()\n",
    "print(\"Answer:\\t\", retrieved_fact)\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(\"Expected:\", expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718ac01",
   "metadata": {},
   "source": [
    "#### <a id='4.2.2'>**4.2.1 Fine-tuned model using the pipeline**</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pipeline\n",
    "pipe_f = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "# Test the Model with the Pipeline\n",
    "print(\"Testing fine-tuned model with pipeline...\")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot that answer basketball questions.\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "# Pass messages directly - cleaner!\n",
    "outputs = pipe_f(\n",
    "    messages, \n",
    "    max_new_tokens=256, \n",
    "    do_sample=True, \n",
    "    temperature=1, \n",
    "    return_full_text=False, \n",
    ")\n",
    "answer = outputs[0][\"generated_text\"]\n",
    "print(\"Answer:\\t\", answer)\n",
    "print(\"‚îÄ\" * 50)\n",
    "print(\"Expected:\", expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7f379",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
